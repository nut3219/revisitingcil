2023-10-16 11:27:53,731 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 11:27:53,731 [trainer.py] => prefix:  
2023-10-16 11:27:53,732 [trainer.py] => dataset: omnibenchmark
2023-10-16 11:27:53,732 [trainer.py] => name: default333
2023-10-16 11:27:53,732 [trainer.py] => memory_size: 0
2023-10-16 11:27:53,732 [trainer.py] => memory_per_class: 0
2023-10-16 11:27:53,732 [trainer.py] => fixed_memory: False
2023-10-16 11:27:53,732 [trainer.py] => shuffle: True
2023-10-16 11:27:53,732 [trainer.py] => init_cls: 30
2023-10-16 11:27:53,732 [trainer.py] => increment: 30
2023-10-16 11:27:53,732 [trainer.py] => model_name: adam_vpt
2023-10-16 11:27:53,732 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-10-16 11:27:53,732 [trainer.py] => device: [device(type='cuda', index=0)]
2023-10-16 11:27:53,732 [trainer.py] => seed: 1993
2023-10-16 11:27:53,732 [trainer.py] => tuned_epoch: 20
2023-10-16 11:27:53,732 [trainer.py] => init_lr: 0.01
2023-10-16 11:27:53,732 [trainer.py] => batch_size: 48
2023-10-16 11:27:53,732 [trainer.py] => weight_decay: 0.0005
2023-10-16 11:27:53,733 [trainer.py] => min_lr: 0
2023-10-16 11:27:53,733 [trainer.py] => optimizer: sgd
2023-10-16 11:27:53,733 [trainer.py] => vpt_type: deep
2023-10-16 11:27:53,733 [trainer.py] => prompt_token_num: 5
2023-10-16 11:27:54,124 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 11:28:06,952 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 11:28:06,952 [trainer.py] => prefix:  
2023-10-16 11:28:06,952 [trainer.py] => dataset: omnibenchmark
2023-10-16 11:28:06,952 [trainer.py] => name: default333
2023-10-16 11:28:06,952 [trainer.py] => memory_size: 0
2023-10-16 11:28:06,952 [trainer.py] => memory_per_class: 0
2023-10-16 11:28:06,952 [trainer.py] => fixed_memory: False
2023-10-16 11:28:06,952 [trainer.py] => shuffle: True
2023-10-16 11:28:06,952 [trainer.py] => init_cls: 30
2023-10-16 11:28:06,952 [trainer.py] => increment: 30
2023-10-16 11:28:06,952 [trainer.py] => model_name: adam_vpt
2023-10-16 11:28:06,952 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21kt
2023-10-16 11:28:06,952 [trainer.py] => device: [device(type='cuda', index=0)]
2023-10-16 11:28:06,953 [trainer.py] => seed: 1993
2023-10-16 11:28:06,953 [trainer.py] => tuned_epoch: 20
2023-10-16 11:28:06,953 [trainer.py] => init_lr: 0.01
2023-10-16 11:28:06,953 [trainer.py] => batch_size: 48
2023-10-16 11:28:06,953 [trainer.py] => weight_decay: 0.0005
2023-10-16 11:28:06,953 [trainer.py] => min_lr: 0
2023-10-16 11:28:06,953 [trainer.py] => optimizer: sgd
2023-10-16 11:28:06,953 [trainer.py] => vpt_type: deep
2023-10-16 11:28:06,953 [trainer.py] => prompt_token_num: 5
2023-10-16 11:28:07,310 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:35:37,461 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:35:37,461 [trainer.py] => prefix:  
2023-10-16 14:35:37,461 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:35:37,461 [trainer.py] => name: default333
2023-10-16 14:35:37,461 [trainer.py] => memory_size: 0
2023-10-16 14:35:37,461 [trainer.py] => memory_per_class: 0
2023-10-16 14:35:37,461 [trainer.py] => fixed_memory: False
2023-10-16 14:35:37,461 [trainer.py] => shuffle: True
2023-10-16 14:35:37,462 [trainer.py] => init_cls: 30
2023-10-16 14:35:37,462 [trainer.py] => increment: 30
2023-10-16 14:35:37,462 [trainer.py] => model_name: adam_vpt
2023-10-16 14:35:37,462 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21kt
2023-10-16 14:35:37,462 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:35:37,462 [trainer.py] => seed: 1993
2023-10-16 14:35:37,462 [trainer.py] => tuned_epoch: 20
2023-10-16 14:35:37,462 [trainer.py] => init_lr: 0.01
2023-10-16 14:35:37,462 [trainer.py] => batch_size: 48
2023-10-16 14:35:37,462 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:35:37,462 [trainer.py] => min_lr: 0
2023-10-16 14:35:37,462 [trainer.py] => optimizer: sgd
2023-10-16 14:35:37,462 [trainer.py] => vpt_type: deep
2023-10-16 14:35:37,462 [trainer.py] => prompt_token_num: 5
2023-10-16 14:35:37,843 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:35:59,787 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:35:59,787 [trainer.py] => prefix:  
2023-10-16 14:35:59,787 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:35:59,788 [trainer.py] => name: default333
2023-10-16 14:35:59,788 [trainer.py] => memory_size: 0
2023-10-16 14:35:59,788 [trainer.py] => memory_per_class: 0
2023-10-16 14:35:59,788 [trainer.py] => fixed_memory: False
2023-10-16 14:35:59,788 [trainer.py] => shuffle: True
2023-10-16 14:35:59,788 [trainer.py] => init_cls: 30
2023-10-16 14:35:59,788 [trainer.py] => increment: 30
2023-10-16 14:35:59,788 [trainer.py] => model_name: adam_vpt
2023-10-16 14:35:59,788 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21kt_vpt
2023-10-16 14:35:59,788 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:35:59,788 [trainer.py] => seed: 1993
2023-10-16 14:35:59,788 [trainer.py] => tuned_epoch: 20
2023-10-16 14:35:59,788 [trainer.py] => init_lr: 0.01
2023-10-16 14:35:59,788 [trainer.py] => batch_size: 48
2023-10-16 14:35:59,788 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:35:59,788 [trainer.py] => min_lr: 0
2023-10-16 14:35:59,788 [trainer.py] => optimizer: sgd
2023-10-16 14:35:59,788 [trainer.py] => vpt_type: deep
2023-10-16 14:35:59,788 [trainer.py] => prompt_token_num: 5
2023-10-16 14:36:00,147 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:37:36,301 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:37:36,302 [trainer.py] => prefix:  
2023-10-16 14:37:36,302 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:37:36,302 [trainer.py] => name: default333
2023-10-16 14:37:36,302 [trainer.py] => memory_size: 0
2023-10-16 14:37:36,302 [trainer.py] => memory_per_class: 0
2023-10-16 14:37:36,302 [trainer.py] => fixed_memory: False
2023-10-16 14:37:36,302 [trainer.py] => shuffle: True
2023-10-16 14:37:36,302 [trainer.py] => init_cls: 30
2023-10-16 14:37:36,302 [trainer.py] => increment: 30
2023-10-16 14:37:36,302 [trainer.py] => model_name: adam_vpt
2023-10-16 14:37:36,302 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21kt_vpt
2023-10-16 14:37:36,302 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:37:36,302 [trainer.py] => seed: 1993
2023-10-16 14:37:36,302 [trainer.py] => tuned_epoch: 20
2023-10-16 14:37:36,302 [trainer.py] => init_lr: 0.01
2023-10-16 14:37:36,303 [trainer.py] => batch_size: 48
2023-10-16 14:37:36,303 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:37:36,303 [trainer.py] => min_lr: 0
2023-10-16 14:37:36,303 [trainer.py] => optimizer: sgd
2023-10-16 14:37:36,303 [trainer.py] => vpt_type: deep
2023-10-16 14:37:36,303 [trainer.py] => prompt_token_num: 5
2023-10-16 14:37:36,659 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:37:45,441 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:37:45,441 [trainer.py] => prefix:  
2023-10-16 14:37:45,441 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:37:45,441 [trainer.py] => name: default333
2023-10-16 14:37:45,441 [trainer.py] => memory_size: 0
2023-10-16 14:37:45,441 [trainer.py] => memory_per_class: 0
2023-10-16 14:37:45,441 [trainer.py] => fixed_memory: False
2023-10-16 14:37:45,441 [trainer.py] => shuffle: True
2023-10-16 14:37:45,441 [trainer.py] => init_cls: 30
2023-10-16 14:37:45,441 [trainer.py] => increment: 30
2023-10-16 14:37:45,441 [trainer.py] => model_name: adam_vpt
2023-10-16 14:37:45,441 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21kt_vpt
2023-10-16 14:37:45,441 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:37:45,441 [trainer.py] => seed: 1993
2023-10-16 14:37:45,442 [trainer.py] => tuned_epoch: 20
2023-10-16 14:37:45,442 [trainer.py] => init_lr: 0.01
2023-10-16 14:37:45,442 [trainer.py] => batch_size: 48
2023-10-16 14:37:45,442 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:37:45,442 [trainer.py] => min_lr: 0
2023-10-16 14:37:45,442 [trainer.py] => optimizer: sgd
2023-10-16 14:37:45,442 [trainer.py] => vpt_type: deep
2023-10-16 14:37:45,442 [trainer.py] => prompt_token_num: 5
2023-10-16 14:37:45,801 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:40:45,848 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:40:45,849 [trainer.py] => prefix:  
2023-10-16 14:40:45,849 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:40:45,849 [trainer.py] => name: default333
2023-10-16 14:40:45,849 [trainer.py] => memory_size: 0
2023-10-16 14:40:45,849 [trainer.py] => memory_per_class: 0
2023-10-16 14:40:45,849 [trainer.py] => fixed_memory: False
2023-10-16 14:40:45,849 [trainer.py] => shuffle: True
2023-10-16 14:40:45,849 [trainer.py] => init_cls: 30
2023-10-16 14:40:45,849 [trainer.py] => increment: 30
2023-10-16 14:40:45,849 [trainer.py] => model_name: adam_vpt
2023-10-16 14:40:45,849 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-10-16 14:40:45,849 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:40:45,849 [trainer.py] => seed: 1993
2023-10-16 14:40:45,849 [trainer.py] => tuned_epoch: 20
2023-10-16 14:40:45,849 [trainer.py] => init_lr: 0.01
2023-10-16 14:40:45,849 [trainer.py] => batch_size: 48
2023-10-16 14:40:45,849 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:40:45,850 [trainer.py] => min_lr: 0
2023-10-16 14:40:45,850 [trainer.py] => optimizer: sgd
2023-10-16 14:40:45,850 [trainer.py] => vpt_type: deep
2023-10-16 14:40:45,850 [trainer.py] => prompt_token_num: 5
2023-10-16 14:40:46,209 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:40:50,842 [trainer.py] => All params: 85844736
2023-10-16 14:40:50,843 [trainer.py] => Trainable params: 46080
2023-10-16 14:40:53,963 [adam_vpt.py] => Learning on 0-30
2023-10-16 14:50:25,178 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.323, Train_accy 90.38, Test_accy 92.67
2023-10-16 14:53:17,383 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:53:17,383 [trainer.py] => prefix:  
2023-10-16 14:53:17,383 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:53:17,383 [trainer.py] => name: default333
2023-10-16 14:53:17,383 [trainer.py] => memory_size: 0
2023-10-16 14:53:17,383 [trainer.py] => memory_per_class: 0
2023-10-16 14:53:17,383 [trainer.py] => fixed_memory: False
2023-10-16 14:53:17,383 [trainer.py] => shuffle: True
2023-10-16 14:53:17,383 [trainer.py] => init_cls: 30
2023-10-16 14:53:17,383 [trainer.py] => increment: 30
2023-10-16 14:53:17,384 [trainer.py] => model_name: adam_vpt
2023-10-16 14:53:17,384 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-10-16 14:53:17,384 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:53:17,384 [trainer.py] => seed: 1993
2023-10-16 14:53:17,384 [trainer.py] => tuned_epoch: 20
2023-10-16 14:53:17,384 [trainer.py] => init_lr: 0.01
2023-10-16 14:53:17,384 [trainer.py] => batch_size: 48
2023-10-16 14:53:17,384 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:53:17,384 [trainer.py] => min_lr: 0
2023-10-16 14:53:17,384 [trainer.py] => optimizer: sgd
2023-10-16 14:53:17,384 [trainer.py] => vpt_type: deep
2023-10-16 14:53:17,384 [trainer.py] => prompt_token_num: 5
2023-10-16 14:53:17,740 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:53:22,284 [trainer.py] => All params: 85844736
2023-10-16 14:53:22,284 [trainer.py] => Trainable params: 46080
2023-10-16 14:53:25,292 [adam_vpt.py] => Learning on 0-30
2023-10-16 14:53:36,121 [trainer.py] => config: ./exps/adam_vpt_deep.json
2023-10-16 14:53:36,122 [trainer.py] => prefix:  
2023-10-16 14:53:36,122 [trainer.py] => dataset: omnibenchmark
2023-10-16 14:53:36,122 [trainer.py] => name: default333
2023-10-16 14:53:36,122 [trainer.py] => memory_size: 0
2023-10-16 14:53:36,122 [trainer.py] => memory_per_class: 0
2023-10-16 14:53:36,122 [trainer.py] => fixed_memory: False
2023-10-16 14:53:36,122 [trainer.py] => shuffle: True
2023-10-16 14:53:36,122 [trainer.py] => init_cls: 30
2023-10-16 14:53:36,122 [trainer.py] => increment: 30
2023-10-16 14:53:36,122 [trainer.py] => model_name: adam_vpt
2023-10-16 14:53:36,122 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-10-16 14:53:36,122 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 14:53:36,122 [trainer.py] => seed: 1993
2023-10-16 14:53:36,122 [trainer.py] => tuned_epoch: 1
2023-10-16 14:53:36,122 [trainer.py] => init_lr: 0.01
2023-10-16 14:53:36,122 [trainer.py] => batch_size: 48
2023-10-16 14:53:36,122 [trainer.py] => weight_decay: 0.0005
2023-10-16 14:53:36,123 [trainer.py] => min_lr: 0
2023-10-16 14:53:36,123 [trainer.py] => optimizer: sgd
2023-10-16 14:53:36,123 [trainer.py] => vpt_type: deep
2023-10-16 14:53:36,123 [trainer.py] => prompt_token_num: 5
2023-10-16 14:53:36,478 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 14:53:41,164 [trainer.py] => All params: 85844736
2023-10-16 14:53:41,165 [trainer.py] => Trainable params: 46080
2023-10-16 14:53:44,236 [adam_vpt.py] => Learning on 0-30
2023-10-16 19:41:45,984 [trainer.py] => config: exps/adam_vpt_deep.json
2023-10-16 19:41:45,984 [trainer.py] => prefix:  
2023-10-16 19:41:45,984 [trainer.py] => dataset: omnibenchmark
2023-10-16 19:41:45,984 [trainer.py] => name: default333
2023-10-16 19:41:45,984 [trainer.py] => memory_size: 0
2023-10-16 19:41:45,984 [trainer.py] => memory_per_class: 0
2023-10-16 19:41:45,984 [trainer.py] => fixed_memory: False
2023-10-16 19:41:45,985 [trainer.py] => shuffle: True
2023-10-16 19:41:45,985 [trainer.py] => init_cls: 30
2023-10-16 19:41:45,985 [trainer.py] => increment: 30
2023-10-16 19:41:45,985 [trainer.py] => model_name: adam_vpt
2023-10-16 19:41:45,985 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-10-16 19:41:45,985 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 19:41:45,985 [trainer.py] => seed: 1993
2023-10-16 19:41:45,985 [trainer.py] => tuned_epoch: 20
2023-10-16 19:41:45,985 [trainer.py] => init_lr: 0.01
2023-10-16 19:41:45,985 [trainer.py] => batch_size: 48
2023-10-16 19:41:45,985 [trainer.py] => weight_decay: 0.0005
2023-10-16 19:41:45,985 [trainer.py] => min_lr: 0
2023-10-16 19:41:45,985 [trainer.py] => optimizer: sgd
2023-10-16 19:41:45,985 [trainer.py] => vpt_type: deep
2023-10-16 19:41:45,985 [trainer.py] => prompt_token_num: 5
2023-10-16 19:41:46,342 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 19:41:51,249 [trainer.py] => All params: 85844736
2023-10-16 19:41:51,249 [trainer.py] => Trainable params: 46080
2023-10-16 19:41:55,642 [adam_vpt.py] => Learning on 0-30
2023-10-16 21:30:32,363 [trainer.py] => config: exps/adam_vpt_deep.json
2023-10-16 21:30:32,364 [trainer.py] => prefix:  
2023-10-16 21:30:32,364 [trainer.py] => dataset: omnibenchmark
2023-10-16 21:30:32,364 [trainer.py] => name: default333
2023-10-16 21:30:32,364 [trainer.py] => memory_size: 0
2023-10-16 21:30:32,364 [trainer.py] => memory_per_class: 0
2023-10-16 21:30:32,364 [trainer.py] => fixed_memory: False
2023-10-16 21:30:32,364 [trainer.py] => shuffle: True
2023-10-16 21:30:32,364 [trainer.py] => init_cls: 30
2023-10-16 21:30:32,364 [trainer.py] => increment: 30
2023-10-16 21:30:32,364 [trainer.py] => model_name: adam_vpt
2023-10-16 21:30:32,364 [trainer.py] => convnet_type: pretrained_vit_b16_224_in21k_vpt
2023-10-16 21:30:32,364 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-10-16 21:30:32,364 [trainer.py] => seed: 1993
2023-10-16 21:30:32,364 [trainer.py] => tuned_epoch: 20
2023-10-16 21:30:32,364 [trainer.py] => init_lr: 0.01
2023-10-16 21:30:32,364 [trainer.py] => batch_size: 48
2023-10-16 21:30:32,365 [trainer.py] => weight_decay: 0.0005
2023-10-16 21:30:32,365 [trainer.py] => min_lr: 0
2023-10-16 21:30:32,365 [trainer.py] => optimizer: sgd
2023-10-16 21:30:32,365 [trainer.py] => vpt_type: deep
2023-10-16 21:30:32,365 [trainer.py] => prompt_token_num: 5
2023-10-16 21:30:32,725 [data_manager.py] => [169, 221, 146, 296, 92, 229, 49, 70, 126, 77, 243, 57, 120, 223, 27, 217, 154, 8, 101, 182, 54, 50, 97, 162, 28, 193, 79, 6, 151, 0, 173, 187, 44, 277, 157, 16, 257, 122, 35, 9, 171, 176, 134, 4, 87, 255, 112, 153, 246, 286, 147, 186, 107, 98, 10, 7, 266, 214, 36, 165, 139, 254, 103, 269, 59, 23, 298, 39, 73, 208, 135, 299, 102, 20, 12, 268, 11, 209, 161, 210, 74, 42, 56, 99, 235, 133, 159, 189, 145, 238, 215, 241, 138, 195, 233, 279, 88, 199, 64, 280, 26, 203, 17, 245, 174, 236, 137, 252, 80, 68, 181, 150, 118, 94, 75, 2, 109, 119, 232, 15, 69, 184, 201, 261, 55, 287, 273, 66, 175, 32, 142, 136, 116, 76, 179, 1, 91, 248, 275, 240, 290, 170, 132, 106, 265, 270, 85, 19, 284, 43, 259, 293, 228, 200, 104, 227, 51, 224, 163, 297, 216, 260, 93, 46, 100, 272, 105, 72, 131, 3, 156, 256, 249, 48, 113, 82, 172, 213, 62, 53, 271, 168, 253, 226, 183, 180, 276, 283, 78, 61, 143, 278, 30, 141, 196, 264, 231, 52, 222, 40, 125, 219, 65, 166, 192, 289, 45, 205, 89, 267, 90, 71, 167, 262, 281, 212, 96, 84, 127, 67, 294, 178, 288, 197, 21, 18, 111, 108, 58, 230, 188, 31, 110, 291, 129, 24, 204, 83, 81, 251, 60, 13, 128, 14, 86, 63, 117, 198, 274, 5, 22, 144, 258, 121, 38, 207, 148, 292, 41, 234, 237, 114, 202, 37, 149, 206, 155, 29, 130, 177, 194, 152, 34, 33, 239, 160, 140, 47, 244, 25, 282, 158, 211, 123, 190, 218, 164, 247, 263, 191, 220, 115, 242, 250, 124, 295, 95, 285, 225, 185]
2023-10-16 21:30:37,316 [trainer.py] => All params: 85844736
2023-10-16 21:30:37,317 [trainer.py] => Trainable params: 46080
2023-10-16 21:30:40,384 [adam_vpt.py] => Learning on 0-30
2023-10-16 21:40:09,113 [adam_vpt.py] => Task 0, Epoch 20/20 => Loss 0.323, Train_accy 90.38, Test_accy 92.67
